ğŸ“„ LangChain RAG Chatbot with PDF & Chat History
```
This project demonstrates a Retrieval-Augmented Generation (RAG) chatbot built using LangChain, OpenAI, and ChromaDB.
The chatbot can:

Load a PDF document

Split it into chunks

Store embeddings in a vector database

Answer questions using retrieved context

Maintain chat history for better follow-up answers
```
ğŸš€ Features
```
ğŸ“„ PDF document loading using PyPDF

âœ‚ï¸ Recursive text splitting

ğŸ§  OpenAI embeddings (text-embedding-3-small)

ğŸ—‚ï¸ Vector storage using Chroma

ğŸ’¬ Context-aware Q&A (RAG)

ğŸ•˜ Chat history aware retriever

ğŸ” Question rewriting for follow-up queries
```
ğŸ› ï¸ Tech Stack
```
Python

LangChain

OpenAI

ChromaDB

Google Colab

PyPDF
```
ğŸ“¦ Installation
```
Install all required packages:

pip install langchain -U
pip install langchain-openai -U
pip install langchain-chroma -U
pip install langchain_community -U
pip install pypdf -U
```
ğŸ”‘ Environment Setup
```
Set your OpenAI API key (Google Colab):

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_KEY")
```
ğŸ“„ Load the PDF
```
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/codeprolk.pdf")
docs = loader.load()
```
âœ‚ï¸ Split the Document
```
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50
)

splits = text_splitter.split_documents(docs)
```
ğŸ§  Create Embeddings & Vector Store
```
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embedding_model
)

retriever = vectorstore.as_retriever()
```
ğŸ¤– Basic RAG Chain
```
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)

system_prompt = """
You are an intelligent chatbot.
Use the following context to answer the question.
If you don't know the answer, say you don't know politely.

{context}
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{question}")
])

rag_chain = (
    {
        "context": retriever,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)
```
ğŸ•˜ Chat History Aware RAG
```
This improves answers for follow-up questions.

Question Rewriting Prompt
from langchain_core.prompts import MessagesPlaceholder

rewrite_prompt = ChatPromptTemplate.from_messages([
    ("system", "Reformulate the question if needed using chat history."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

History-Aware Retriever
history_aware_retriever = (
    rewrite_prompt
    | llm
    | (lambda x: x.content)
    | retriever
)
```
ğŸ’¬ Final RAG Chain with Memory
```
from langchain_core.runnables import RunnableLambda

final_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

rag_chain = (
    {
        "context": history_aware_retriever,
        "input": RunnablePassthrough(),
        "chat_history": RunnableLambda(lambda x: x["chat_history"])
    }
    | final_prompt
    | llm
)
```

ğŸ§ª Example Usage
```
from langchain_core.messages import HumanMessage, AIMessage

chat_history = []

question = "Can you describe it briefly?"

response = rag_chain.invoke({
    "input": question,
    "chat_history": chat_history
})

print(response.content)

chat_history.append(HumanMessage(content=question))
chat_history.append(AIMessage(content=response.content))
```

ğŸ“Œ Use Cases
```
Chat with PDF documents

Knowledge-based assistants

Company profile chatbots

Research document Q&A

Student project / internship portfolio
```

ğŸ™Œ Author
```
Rusira DinuJaya
Software Engineering Intern | LangChain & AI Enthusiast
```
